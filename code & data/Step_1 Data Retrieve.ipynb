{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCiXrok9kl7f"
      },
      "source": [
        "# Data Retrieve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC7_8J_Akqo3"
      },
      "source": [
        "## Events\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifrKq33brDDn"
      },
      "source": [
        "### Events list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYn6_IfMV0vI"
      },
      "source": [
        "Retrieved through Octopus, and you can access the processes by using the project file: Event_list.otd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkaFrHoFrRBR"
      },
      "source": [
        "### Events details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoY5puJbsYaS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-w_af92C6k8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./data/Event_list.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "# List of User-Agent strings to rotate\n",
        "user_agents = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/119.0',\n",
        "]\n",
        "\n",
        "# Function to get a random User-Agent\n",
        "def get_random_user_agent():\n",
        "    return random.choice(user_agents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye7nGQxcH66P"
      },
      "outputs": [],
      "source": [
        "def process_batch(urls, batch_index):\n",
        "    # Dictionaries to store the events details and errors\n",
        "    events = {}\n",
        "    errors = {}\n",
        "    # Loop through each URL\n",
        "    for url in urls:\n",
        "        try:\n",
        "            # Set headers with a random User-Agent\n",
        "            headers = {'User-Agent': get_random_user_agent()}\n",
        "\n",
        "            # Send an HTTP request to the URL\n",
        "            response = requests.get(url, headers=headers)\n",
        "\n",
        "            # Parse the HTML content\n",
        "            tree = html.fromstring(response.content)\n",
        "\n",
        "            # Use XPath to extract the content of the specific element\n",
        "            element_content = tree.xpath('/html/body/script[1]/text()')\n",
        "\n",
        "            # If content is successfully extracted, save it in the events dictionary\n",
        "            if element_content:\n",
        "                parsed_data = json.loads(element_content[0])\n",
        "                events[url] = parsed_data\n",
        "            else:\n",
        "                errors[url] = \"No content found\"\n",
        "\n",
        "        # In case of an error, log the error message\n",
        "        except Exception as e:\n",
        "            errors[url] = str(e)\n",
        "        time.sleep(random.uniform(1, 5))\n",
        "\n",
        "    # Save results and errors\n",
        "    if not os.path.exists(f'./data/temp/events_details/{batch_index}'):\n",
        "        os.makedirs(f'./data/temp/events_details/{batch_index}')\n",
        "    with open(f'./data/temp/events_details/{batch_index}/results.json', 'w') as f:\n",
        "        json.dump(events, f, indent=2)\n",
        "    with open(f'./data/temp/events_details/{batch_index}/errors.json', 'w') as f:\n",
        "        json.dump(errors, f, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWHnuTJiI62p"
      },
      "outputs": [],
      "source": [
        "# Split URLs into batches of 100 each\n",
        "urls = df['URL'].tolist()\n",
        "batch_size = 100\n",
        "url_batches = [urls[i:i + batch_size] for i in range(0, len(urls), batch_size)]\n",
        "\n",
        "# Process each batch\n",
        "for index, batch in enumerate(url_batches):\n",
        "    process_batch(batch, index)\n",
        "    print(f\"Batch {index} processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQiXihFEJU57",
        "outputId": "2d75a189-09a3-4b18-d726-dc340b2dffea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All batches processed.\n"
          ]
        }
      ],
      "source": [
        "# Merge results\n",
        "final_results = {}\n",
        "final_errors = {}\n",
        "\n",
        "for i in range(len(url_batches)):\n",
        "    with open(f'./data/temp/events_details/{i}/results.json') as f:\n",
        "        final_results.update(json.load(f))\n",
        "    with open(f'./data/temp/events_details/{i}/errors.json') as f:\n",
        "        final_errors.update(json.load(f))\n",
        "\n",
        "# Save final results and errors\n",
        "with open('./data/events_details.json', 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "with open('./temp/events_details_errors.json', 'w') as f:\n",
        "    json.dump(final_errors, f, indent=2)\n",
        "\n",
        "print(\"All batches processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cWVHG0w02A1"
      },
      "source": [
        "### Event Categories - Gamma API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AivClD5w04gn",
        "outputId": "43f9b22a-1b89-4d5c-d388-1730f8ea102a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done.\n"
          ]
        }
      ],
      "source": [
        "url = \"https://gamma-api.polymarket.com/categories\"\n",
        "\n",
        "# Send a GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse JSON response\n",
        "    data = response.json()\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open('./data/Categories.json', 'w') as file:\n",
        "        json.dump(data, file, indent=2)\n",
        "    print(\"Done.\")\n",
        "else:\n",
        "    print(f\"Failed to fetch data. Status code: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiaGosAbUZ2C"
      },
      "source": [
        "## Market & Participants - Subgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKnK8BxExRB1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://api.studio.thegraph.com/query/63814/pm_analysis/v0.0.1\" #Subgraph API\n",
        "log_file = './temp/market_user_request_errors.log'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz1Wsl9c0juk"
      },
      "source": [
        "### Retrieve all User overview data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKEtzX6k0lIB"
      },
      "outputs": [],
      "source": [
        "def send_request(url, query, last_timestamp, log_file, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, json={'query': query}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = json.loads(response.text)\n",
        "            if 'errors' in data:\n",
        "                print(f\"Error in response: {data['errors']}\")\n",
        "                with open(log_file, 'a') as log:\n",
        "                    log.write(f\"Error with timestamp {last_timestamp}, response errors: {data['errors']}\\n\")\n",
        "                return None\n",
        "            return data\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error, retry {attempt+1}/{max_retries}: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                with open(log_file, 'a') as log:\n",
        "                    # log error and last timestamp\n",
        "                    log.write(f\"Failed to fetch data after timestamp: {last_timestamp}, error: {e}\\n\")\n",
        "                return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwsO5IVw04zL"
      },
      "outputs": [],
      "source": [
        "markets_user_data_all = []\n",
        "last_timestamp = 1599166595  # Minimum known timestamp\n",
        "max_timestamp = 1708985858  # Maximum known timestamp\n",
        "max_data = False  # Flag to indicate when to stop the loop\n",
        "\n",
        "while not max_data and last_timestamp <= max_timestamp:\n",
        "    query = \"\"\"\n",
        "    {{\n",
        "        accounts(\n",
        "            where: {{creationTimestamp_gt: {}}}\n",
        "            first: 1000  # 1000 is the max number of accounts per request\n",
        "            orderBy: creationTimestamp\n",
        "            orderDirection: asc\n",
        "        ) {{\n",
        "            id\n",
        "            creationTimestamp\n",
        "            lastSeenTimestamp\n",
        "            numTrades\n",
        "            scaledCollateralVolume\n",
        "            lastTradedTimestamp\n",
        "            scaledProfit\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\".format(last_timestamp)\n",
        "\n",
        "    data = send_request(url, query, last_timestamp, log_file)  # Send the request and get the response data\n",
        "    if data is None:\n",
        "        max_data = True\n",
        "    else:\n",
        "        accounts_data = data.get('data', {}).get('accounts')\n",
        "        if accounts_data:\n",
        "            markets_user_data_all.extend(accounts_data)\n",
        "            print(f\"Retrieved {len(markets_user_data_all)} accounts\")\n",
        "            last_timestamp = int(accounts_data[-1]['creationTimestamp'])  # Update last_timestamp\n",
        "        else:\n",
        "            max_data = True  # No more data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68ubR_UF06ak"
      },
      "outputs": [],
      "source": [
        "#Save all market user list\n",
        "with open('./data/markets_user_data_all.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(markets_user_data_all, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1SsPuOIxX7y"
      },
      "source": [
        "### Retrieve User list in each politics market"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjndCiN5AJy5"
      },
      "outputs": [],
      "source": [
        "def send_request(url, query, market_id, clobTokenId, skip, log_file, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, json={'query': query}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            return json.loads(response.text)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error, retry {attempt+1}/{max_retries}: {e}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                # Record errors\n",
        "                with open(log_file, 'a') as log:\n",
        "                    log.write(f\"Failed to fetch data for market_id: {market_id}, clobTokenId: {clobTokenId}, skip: {skip}, error: {e}\\n\")\n",
        "                return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNhRlmDz053J"
      },
      "outputs": [],
      "source": [
        "with open('./data/politics_markets.json', 'r', encoding='utf-8') as file:\n",
        "    politics_markets_data = json.load(file)\n",
        "\n",
        "# Extract user information for each market\n",
        "markets_user_data = []\n",
        "\n",
        "for event in politics_markets_data.values():\n",
        "    event_data = event['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']\n",
        "\n",
        "    for market in event_data.get('markets', []):\n",
        "        market_id = market.get('id')\n",
        "        condition_id = market.get('conditionId')\n",
        "        clobTokenIds = market.get('clobTokenIds', [])\n",
        "\n",
        "        if clobTokenIds:\n",
        "            clobTokenId = clobTokenIds[0]\n",
        "            accounts = []\n",
        "            skip = 0\n",
        "\n",
        "            while True:\n",
        "                # GraphQL\n",
        "                query = \"\"\"\n",
        "                {{\n",
        "                    accounts(\n",
        "                        where: {{marketPositions_: {{market: \"{}\"}}}}\n",
        "                        skip: {}\n",
        "                    ) {{\n",
        "                        id\n",
        "                    }}\n",
        "                }}\n",
        "                \"\"\".format(clobTokenId, skip)\n",
        "\n",
        "                data = send_request(url, query, market_id, clobTokenId, skip, log_file)\n",
        "                if data is None:\n",
        "                    break  # If failed, next clobTokenId\n",
        "\n",
        "                accounts_batch = data['data']['accounts']\n",
        "                if not accounts_batch:\n",
        "                    break\n",
        "\n",
        "                accounts.extend([account['id'] for account in accounts_batch])\n",
        "                skip += 100\n",
        "\n",
        "            # Add users data\n",
        "            markets_user_data.append({\n",
        "                \"id\": market_id,\n",
        "                \"conditionId\": condition_id,\n",
        "                \"accounts\": accounts\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UO9L5AEDBObo"
      },
      "outputs": [],
      "source": [
        "#Save political market user list\n",
        "with open('./data/temp/politics_markets_user_data.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(markets_user_data, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo0N-lf42TlE"
      },
      "source": [
        "### Retrieve failed market"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G2Y_RUJUeib"
      },
      "outputs": [],
      "source": [
        "with open('./data/temp/politics_markets_user_data.json', 'r', encoding='utf-8') as file:\n",
        "    markets_user_data = json.load(file)\n",
        "\n",
        "with open('./data/temp/Politics_market_user_request_errors.log', 'r') as log_file:\n",
        "    failed_requests = log_file.readlines()\n",
        "\n",
        "# Retrieve agian\n",
        "for failed_request in failed_requests:\n",
        "    parts = failed_request.strip().split(\", \", 3)\n",
        "    market_id = parts[0].split(\": \")[1]\n",
        "    clobTokenId = parts[1].split(\": \")[1]\n",
        "    skip = int(parts[2].split(\": \")[1])\n",
        "\n",
        "    query = \"\"\"\n",
        "    {{\n",
        "        accounts(\n",
        "            where: {{marketPositions_: {{market: \"{}\"}}}}\n",
        "            skip: {}\n",
        "        ) {{\n",
        "            id\n",
        "        }}\n",
        "    }}\n",
        "    \"\"\".format(clobTokenId, skip)\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json={'query': query}, timeout=30)  # More timeout\n",
        "        response.raise_for_status()\n",
        "        data = json.loads(response.text)\n",
        "        accounts_batch = data['data']['accounts']\n",
        "\n",
        "        if not accounts_batch:\n",
        "            break\n",
        "\n",
        "        # update markets_user_data\n",
        "        for market in markets_user_data:\n",
        "            if market['id'] == market_id:\n",
        "                market['accounts'].extend([account['id'] for account in accounts_batch])\n",
        "                break\n",
        "        skip += 100\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Retry failed: {e}\")\n",
        "\n",
        "# Save update Json\n",
        "with open('./data/politics_markets_user_data_updated.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(markets_user_data, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la1lsgV8VTS0"
      },
      "source": [
        "### Subgraph not includes data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-8L2rlYWNrE"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import requests\n",
        "from lxml import html\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUshVcxu8XP3"
      },
      "source": [
        "#### Obtain markets with empty accounts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH50Pw-r8Wxi"
      },
      "outputs": [],
      "source": [
        "# From subgraph and Web retrieve\n",
        "with open('./data/politics_markets_user_data_updated.json', 'r', encoding='utf-8') as file:\n",
        "    markets_user_data_updated = json.load(file)\n",
        "# From Web retrieve\n",
        "with open('/data/politics_markets.json', 'r', encoding='utf-8') as file:\n",
        "    politics_markets_data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DxfGjJZBd5o"
      },
      "outputs": [],
      "source": [
        "# Find the subgraph missed markets id\n",
        "empty_accounts_ids = [item['id'] for item in markets_user_data_updated if not item['accounts']]\n",
        "\n",
        "matched_markets = {}\n",
        "\n",
        "# Obtain related data\n",
        "for event_url, event_data in politics_markets_data.items():\n",
        "    markets = event_data.get('props', {}).get('pageProps', {}).get('dehydratedState', {}).get('queries', [])\n",
        "    for query in markets:\n",
        "        for market in query.get('state', {}).get('data', {}).get('markets', []):\n",
        "            if market['id'] in empty_accounts_ids:\n",
        "                if event_url not in matched_markets:\n",
        "                    matched_markets[event_url] = {'markets': []}\n",
        "                matched_markets[event_url]['markets'].append({\n",
        "                    'id': market['id'],\n",
        "                    'question': market['question'],\n",
        "                    'conditionId': market['conditionId'],\n",
        "                    'slug': market['slug'],\n",
        "                    'clobTokenIds': market['clobTokenIds']\n",
        "                })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZVQkKATKfLm"
      },
      "outputs": [],
      "source": [
        "# Save the temp json\n",
        "with open('./data/temp/politics_markets_user_data_subgraphmissed.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(matched_markets, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Save the temp csv\n",
        "df = pd.DataFrame(matched_markets)\n",
        "df.to_csv('./data/temp/politics_markets_user_data_subgraphmissed.csv', index=False)\n",
        "\n",
        "# Save the temp market url csv\n",
        "with open('./data/temp/politics_markets_user_data_subgraphmissed.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    for key in matched_markets.keys():\n",
        "        csvwriter.writerow([key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wflouFV3DUzr"
      },
      "source": [
        "#### Retrieve lost data through octopus and integrate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50bsUlGyNwDX"
      },
      "source": [
        "You can use the Octopus app with the project file:\n",
        "\n",
        "Subgraphmissed.otd\n",
        "\n",
        "to obtain the lost data Subgraphmissed.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8GL5zVOFBp"
      },
      "outputs": [],
      "source": [
        "# Load the supplement data\n",
        "subgraph_missed = pd.read_csv('./data/temp/Subgraphmissed.csv')\n",
        "subgraph_missed['URL'] = subgraph_missed['URL'].apply(lambda x: x.lstrip('\\ufeff'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohq6cMcdQ_vQ"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "# Exclude the just 0 betting volume markets, which have effective peer markets\n",
        "urls_to_exclude = []\n",
        "for url, data in matched_markets.items():\n",
        "    matched_markets_length = len(data['markets'])\n",
        "    politics_markets_length = len(politics_markets_data[url]['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['markets'])\n",
        "    if matched_markets_length < politics_markets_length:\n",
        "        urls_to_exclude.append(url)\n",
        "\n",
        "# Delete null lines\n",
        "subgraph_missed.dropna(subset=['Market'], inplace=True)\n",
        "\n",
        "# Delete final blank\n",
        "subgraph_missed['Market'] = subgraph_missed['Market'].str.strip()\n",
        "\n",
        "# Special processing\n",
        "special_url1 = 'https://polymarket.com/event/electoral-college-margin-of-victory-in-the-2024-presidential-election'\n",
        "special_url2 = 'https://polymarket.com/event/biden-disapproval-on-dec-29'\n",
        "special_url3 = 'https://polymarket.com/event/el-salvador-presidential-election-winner'\n",
        "special_url4 = 'https://polymarket.com/event/republican-nominee-2024'\n",
        "special_url5 = 'https://polymarket.com/event/biden-approval-on-jan-7'\n",
        "special_url6 = 'https://polymarket.com/event/democratic-nominee-2024'\n",
        "special_url7 = 'https://polymarket.com/event/finland-presidential-election-who-will-win'\n",
        "special_url8 = 'https://polymarket.com/event/iowa-caucus-2nd-place'\n",
        "special_url9 = 'https://polymarket.com/event/new-hampshire-democratic-primary-winner'\n",
        "special_url10 = 'https://polymarket.com/event/presidential-election-popular-vote-winner-2024'\n",
        "special_url11 = 'https://polymarket.com/event/presidential-election-winner-2024'\n",
        "special_url12 = 'https://polymarket.com/event/trump-margin-of-victory-in-iowa-caucus'\n",
        "\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url1, 'Market'] = subgraph_missed['Market'].str.replace('GOP/Dems ', '')\\\n",
        "                                                                        .str.replace('Dems ', 'Democrats win ').str.replace('GOP ', 'GOP wins ')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url2, 'Market'] = subgraph_missed['Market'].str.replace('>', 'greater than ')\\\n",
        "                                                                        .str.replace('<', 'less than ').str.replace(' - ', ' and ')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url3, 'Market'] = subgraph_missed['Market'].str.replace('Sánchez', 'Sanchez')\\\n",
        "                                                                        .str.replace('Other', 'someone else')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url4, 'Market'] = subgraph_missed['Market'].str.replace(r'Donald Trump(?!\\sJr\\.)', 'Donald J. Trump', regex=True)\\\n",
        "                                                                        .str.replace('Other', 'someone else')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url5, 'Market'] = subgraph_missed['Market'].str.replace('>', 'greater than ')\\\n",
        "                                                                        .str.replace('<', 'less than ')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url6, 'Market'] = subgraph_missed['Market'].str.replace('Other', 'someone else')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url7, 'Market'] = subgraph_missed['Market'].str.replace('Väyrynen', 'Vayrynen')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url8, 'Market'] = subgraph_missed['Market'].str.replace('Donald Trump', 'Trump')\\\n",
        "                                                                        .str.replace('Nikki Haley', 'Haley').str.replace('Other', 'someone else')\\\n",
        "                                                                        .str.replace('Ron DeSantis', 'DeSantis').str.replace('Vivek Ramaswamy', 'Ramaswamy')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url9, 'Market'] = subgraph_missed['Market'].str.replace('Joe Biden (Write-in)', 'Biden').str.replace('Other', 'another')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url10, 'Market'] = subgraph_missed['Market'].str.replace('Robert F. Kennedy', 'RFK')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url11, 'Market'] = subgraph_missed['Market'].str.replace('Democrat ', 'Democratic ')\n",
        "subgraph_missed.loc[subgraph_missed['URL'] == special_url12, 'Market'] = subgraph_missed['Market'].str.replace('0-20%', '<20%').str.replace(' Loses', ' lose')\n",
        "\n",
        "\n",
        "subgraph_missed_list = subgraph_missed.to_dict('records')\n",
        "# Log failed\n",
        "unmatched_urls_log = []\n",
        "\n",
        "for missed in subgraph_missed_list:\n",
        "    url = missed['URL']\n",
        "    # Ignore the excluding urls\n",
        "    if url in urls_to_exclude:\n",
        "        continue\n",
        "\n",
        "    market_name = str(missed['Market']).lower().strip()\n",
        "    address = missed['Address']\n",
        "\n",
        "    # Match markets\n",
        "    if url in matched_markets:\n",
        "        market_found = False\n",
        "        for market in matched_markets[url]['markets']:\n",
        "            # Match market name within question\n",
        "            if market_name in str(market['question']).lower():\n",
        "                market_id = market['id']\n",
        "                market_found = True\n",
        "                # Upgrade accounts\n",
        "                for user_data in markets_user_data_updated:\n",
        "                    if user_data['id'] == market_id:\n",
        "                        if address not in user_data['accounts']:\n",
        "                            user_data['accounts'].append(address)\n",
        "                        break\n",
        "    # For those market names not matched\n",
        "        if not market_found:\n",
        "            unmatched_urls_log.append(f\"Name not match, URL: {url}, Market Name: {missed['Market']}\")\n",
        "\n",
        "    # For those not find urls\n",
        "    else:\n",
        "        unmatched_urls_log.append(f\"URL not found, URL: {url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5apYkvbdnKqs"
      },
      "outputs": [],
      "source": [
        "# Log\n",
        "unmatched_urls_log = list(set(unmatched_urls_log))\n",
        "with open('./data/temp/unmatched_urls_log.txt', 'w', encoding='utf-8') as log_file:\n",
        "    for url in sorted(unmatched_urls_log):\n",
        "        log_file.write(url + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ce6km_C5cJr4"
      },
      "outputs": [],
      "source": [
        "# Upgrade markets_user_data_updated\n",
        "# Use this updated data for EDA's Participants calculate and all other user retrieve with politics markets\n",
        "with open('./data/politics_markets_user_data_updated.json', 'w', encoding='utf-8') as file: \n",
        "    json.dump(markets_user_data_updated, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mku89dqCPxvi"
      },
      "source": [
        "## Participants and Transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DLdC0Kf9AJ2"
      },
      "source": [
        "### Obtain the participants list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF7PceanQkyw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import os\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i5Qh69-P-jH"
      },
      "outputs": [],
      "source": [
        "# From subgraph, Web retrieve, and supplimentary\n",
        "with open('./data/politics_markets_user_data_updated.json', 'r', encoding='utf-8') as file:\n",
        "    markets_user_data_updated = json.load(file)\n",
        "\n",
        "# For retrieve from subgraph\n",
        "unique_accounts = set()\n",
        "for entry in markets_user_data_updated:\n",
        "    for account in entry[\"accounts\"]:\n",
        "        unique_accounts.add(account)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqy1WSOyRDy0"
      },
      "outputs": [],
      "source": [
        "# For retrieve from webpage\n",
        "prefixed_accounts = [\"https://polymarket.com/profile/\" + account for account in unique_accounts]\n",
        "\n",
        "with open('./data/temp/politics_accounts.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    for account in prefixed_accounts:\n",
        "        writer.writerow([account])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nDvXat8jxQk"
      },
      "source": [
        "### API - UserData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0JzyUXFXFz-d"
      },
      "outputs": [],
      "source": [
        "# Obtain the unique political markets participants' profiles and activities\n",
        "unique_accounts = list(unique_accounts)\n",
        "batch_size = 100\n",
        "\n",
        "for i in range(0, len(unique_accounts), batch_size):\n",
        "    batch_addresses = unique_accounts[i:i+batch_size]\n",
        "    results = {}\n",
        "    for address in batch_addresses:\n",
        "\n",
        "        # initial\n",
        "        combined_data = {\"profile\": {}, \"profit\": {}, \"marketsTraded\": \"\", \"volumeTraded\": \"\", \"subgraph\": {}, \"positions\": [], \"activities\": []}\n",
        "\n",
        "        # API URLs\n",
        "        profile_url = f\"https://polymarket.com/api/profile/userData?address={address}\"\n",
        "        profit_url = f\"https://polymarket.com/api/profile/profit?address={address}\"\n",
        "        markets_traded_url = f\"https://polymarket.com/api/profile/marketsTraded?address={address}\"\n",
        "        subgraph_account_data_url = f\"https://polymarket.com/api/profile/subgraphAccountData?address={address}\"\n",
        "        volume_traded_url = f\"https://polymarket.com/api/profile/volume?range=all&address={address}\"\n",
        "        positions_url = f\"https://polymarket.com/api/profile/positions?user={address}\"\n",
        "        activity_url_template = \"https://polymarket.com/api/profile/activity?user={address}&limit=100&offset={offset}\"\n",
        "\n",
        "        # request APIs\n",
        "        try:\n",
        "            profile_data = requests.get(profile_url, timeout=10).json()\n",
        "            if not isinstance(profile_data, dict):\n",
        "                profile_data = {}\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            profile_data = {}\n",
        "        try:\n",
        "            profit_data = requests.get(profit_url, timeout=10).json()\n",
        "            if not isinstance(profit_data, dict):\n",
        "                profit_data = {}\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            profit_data = {}\n",
        "\n",
        "        try:\n",
        "            markets_traded_data = requests.get(markets_traded_url, timeout=10).json()\n",
        "            if not isinstance(markets_traded_data, dict):\n",
        "                markets_traded_data = {}\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            markets_traded_data = {}\n",
        "\n",
        "        try:\n",
        "            subgraph_account_data = requests.get(subgraph_account_data_url, timeout=10).json()\n",
        "            if not isinstance(subgraph_account_data, dict):\n",
        "                subgraph_account_data = {}\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            subgraph_account_data = {}\n",
        "\n",
        "        try:\n",
        "            volume_traded_data = requests.get(volume_traded_url, timeout=10).json()\n",
        "            if not isinstance(volume_traded_data, dict):\n",
        "                volume_traded_data = {}\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            volume_traded_data = {}\n",
        "\n",
        "        combined_data['profile'] = {k: profile_data.get(k, None) for k in profile_data if k not in ['displayUsernamePublic', 'profileImage', 'profileImageOptimized', 'users', '__typename']}\n",
        "        combined_data['profit'] = {k: profit_data.get(k, None) for k in ['realized', 'unrealized', 'total']}\n",
        "        combined_data['marketsTraded'] = int(markets_traded_data.get('count', 0))\n",
        "        combined_data['volumeTraded'] = volume_traded_data.get('amount', 0)\n",
        "        combined_data['subgraph'] = {k: subgraph_account_data.get(k, None) for k in subgraph_account_data if k not in ['vid', 'block_range', 'id', '_gs_chain', '_gs_gid']}\n",
        "\n",
        "        # Positions\n",
        "        try:\n",
        "            positions_response = requests.get(positions_url, timeout=10).json()\n",
        "            if not isinstance(positions_response, list):\n",
        "                positions_response = []  # If it is not a list, give it []\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            positions_response = []\n",
        "\n",
        "        for position in positions_response:\n",
        "            position_data = {k: v for k, v in position.items() if k not in ['proxyWallet', 'icon']}\n",
        "            combined_data['positions'].append(position_data)\n",
        "\n",
        "        # Activities\n",
        "        offset = 0\n",
        "        while True:\n",
        "            try:\n",
        "                activity_url = activity_url_template.format(address=address, offset=offset)\n",
        "                activity_response = requests.get(activity_url, timeout=10).json()\n",
        "                if not isinstance(activity_response, list):\n",
        "                    activity_response = []\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                activity_response = []\n",
        "\n",
        "            if not activity_response:\n",
        "                break  # No more data\n",
        "            for activity in activity_response:\n",
        "                activity_data = {k: v for k, v in activity.items() if k not in ['proxyWallet', 'icon', 'name', 'pseudonym', 'bio', 'profileImage', 'profileImageOptimized']}\n",
        "                combined_data['activities'].append(activity_data)\n",
        "            offset += 100\n",
        "\n",
        "        # Add the combined data to the results dictionary\n",
        "        key = 'https://polymarket.com/profile/' + address\n",
        "        results[key] = combined_data\n",
        "\n",
        "\n",
        "    # Save batch jason\n",
        "    batch_filename = f'./data/temp/politics_users_profile_api_batch_{i//batch_size + 1}.json'\n",
        "    with open(batch_filename, 'w', encoding='utf-8') as file:\n",
        "        json.dump(results, file, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # update log\n",
        "    with open('./data/temp/log.txt', 'a') as log_file:\n",
        "        for address in batch_addresses:\n",
        "            log_file.write(f'{address} \\n')\n",
        "\n",
        "    print(f'Batch {i//batch_size + 1} processed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSwR9avLvxsp"
      },
      "outputs": [],
      "source": [
        "# Combine Json and preprocess\n",
        "folder_path = './data/temp/'\n",
        "combined_data = defaultdict(dict)\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.json'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            for address, info in data.items():\n",
        "                combined_data[address].update(info)\n",
        "\n",
        "# calculate \"activities\"\n",
        "for address, info in combined_data.items():\n",
        "    unique_activities = {each['transactionHash']: each for each in info.get('activities', [])}.values()\n",
        "    combined_data[address]['activities'] = list(unique_activities)\n",
        "    combined_data[address]['activityNum'] = len(unique_activities)\n",
        "\n",
        "\n",
        "for address, info in combined_data.items():\n",
        "    # Update \"profile\" - \"createdAt\"\n",
        "    if not info['profile'].get('createdAt'):\n",
        "        creation_timestamp = info['subgraph'].get('creation_timestamp')\n",
        "        if creation_timestamp:\n",
        "            # Transfer timestamp to 'YYYY-MM-DDTHH:MM:SS.SSSZ'\n",
        "            creation_datetime = datetime.utcfromtimestamp(int(creation_timestamp)).strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
        "            info['profile']['createdAt'] = creation_datetime\n",
        "        else:\n",
        "            info['profile']['createdAt'] = None\n",
        "\n",
        "    # Update \"profile\" - \"proxyWallet\"\n",
        "    if not info['profile'].get('proxyWallet'):\n",
        "        proxy_wallet = urlparse(address).path.split('/')[-1]\n",
        "        info['profile']['proxyWallet'] = proxy_wallet\n",
        "\n",
        "    # Update \"profit\" - \"total\"\n",
        "    if info['profit'].get('total') is None:\n",
        "        info['profit']['total'] = float(info['subgraph'].get('scaled_profit', None))\n",
        "\n",
        "    # Update \"marketsTraded\"\n",
        "    if info.get('marketsTraded') == 0:\n",
        "        condition_ids = set()\n",
        "        for position in info.get('positions', []):\n",
        "            condition_ids.add(position.get('conditionId'))\n",
        "        for activity in info.get('activities', []):\n",
        "            condition_ids.add(activity.get('conditionId'))\n",
        "        info['marketsTraded'] = len(condition_ids)\n",
        "\n",
        "# Save the combined Json\n",
        "output_file_path = './data/politics_users_profile_api.json'\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(combined_data, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBJTfUDhGVj0"
      },
      "source": [
        "### Polygon API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYGtt6jIqCUE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import pickle as pkl\n",
        "from google.colab import userdata #change the API for your own\n",
        "import time\n",
        "import concurrent.futures\n",
        "from collections import defaultdict\n",
        "import queue\n",
        "import logging\n",
        "import threading\n",
        "api_key = userdata.get('polygon1')\n",
        "api_key2 = userdata.get('polygon2')\n",
        "api_key3 = userdata.get('polygon3')\n",
        "api_key4 = userdata.get('polygon4')\n",
        "api_key5 = userdata.get('polygon5')\n",
        "api_key6 = userdata.get('polygon6')\n",
        "\n",
        "api_keys = [api_key, api_key2, api_key3, api_key4, api_key5, api_key6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNHJnbdFIfD4"
      },
      "outputs": [],
      "source": [
        "with open('./data/politics_users_profile_api.json', 'r', encoding='utf-8') as file:\n",
        "    politics_users_profile = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlhajrDpLYLF"
      },
      "outputs": [],
      "source": [
        "filtered_data = []\n",
        "for user, user_data in politics_users_profile.items():\n",
        "    if \"activities\" in user_data:\n",
        "        activities = user_data[\"activities\"]\n",
        "        redeem_condition_ids = set()\n",
        "        non_redeem_condition_ids = set()\n",
        "\n",
        "        for activity in activities:\n",
        "            if activity[\"type\"] == \"REDEEM\":\n",
        "                redeem_condition_ids.add(activity[\"conditionId\"])\n",
        "            else:\n",
        "                non_redeem_condition_ids.add(activity[\"conditionId\"])\n",
        "\n",
        "        common_condition_ids = redeem_condition_ids & non_redeem_condition_ids\n",
        "\n",
        "        if common_condition_ids:\n",
        "            user_data[\"activities\"] = [activity for activity in activities if activity[\"conditionId\"] not in common_condition_ids]\n",
        "        user_data[\"activities\"] = [activity for activity in user_data[\"activities\"] if activity[\"type\"] == \"REDEEM\"]\n",
        "\n",
        "    if \"activities\" not in user_data or len(user_data[\"activities\"]) > 0:\n",
        "        filtered_data.append({user: user_data})\n",
        "\n",
        "with open('./data/temp/filtered_politics_users_profile_temp.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(filtered_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "filtered_proxy_wallets = []\n",
        "for user_data in filtered_data:\n",
        "    for user, data in user_data.items():\n",
        "        if \"profile\" in data and \"proxyWallet\" in data[\"profile\"]:\n",
        "            filtered_proxy_wallets.append(data[\"profile\"][\"proxyWallet\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQVkUp8K0-hQ"
      },
      "outputs": [],
      "source": [
        "exist_hash = {}\n",
        "for user_url, user_data in politics_users_profile.items():\n",
        "    if \"profile\" in user_data and \"proxyWallet\" in user_data[\"profile\"]:\n",
        "        proxy_wallet = user_data[\"profile\"][\"proxyWallet\"]\n",
        "        if proxy_wallet in filtered_proxy_wallets:\n",
        "            if \"activities\" in user_data:\n",
        "                transaction_hashes = [activity[\"transactionHash\"] for activity in user_data[\"activities\"]]\n",
        "                exist_hash[proxy_wallet] = transaction_hashes\n",
        "            else:\n",
        "                exist_hash[proxy_wallet] = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDgbh4PdUeaS"
      },
      "outputs": [],
      "source": [
        "api_type = \"https://api.polygonscan.com/api?module=logs&action=getLogs\"\n",
        "asset_address = \"0x4d97dcd97ec945f40cf65f87097ace5ea0476045\"  # The smart contract of the betting\n",
        "topic_0 = \"0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62\" # ERC1155 TransferSingle event\n",
        "start_block = 9000000  # 2021\n",
        "end_block = 54000000    # 2024-02-26\n",
        "RATE_LIMIT = 5  # 5 requests per second\n",
        "\n",
        "def get_transaction_hashes(user_address, from_block, to_block, api_key):\n",
        "    url = f\"{api_type}&fromBlock={from_block}&toBlock={to_block}&address={asset_address}&topic0={topic_0}&topic0_2_opr=and&topic2=0x000000000000000000000000{user_address[2:]}&topic2_3_opr=or&topic3=0x000000000000000000000000{user_address[2:]}&apikey={api_key}\"\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            time.sleep(1 / RATE_LIMIT)\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()#[\"result\"]\n",
        "            #print(data)\n",
        "            if isinstance(data, str):  # If data is a string, there was an error\n",
        "                #print(f\"Str Error: {data}\")\n",
        "                return []\n",
        "\n",
        "            if data[\"status\"] == \"0\": #and data[\"message\"] == \"No records found\"\n",
        "                return []\n",
        "            logs = data[\"result\"]\n",
        "            #print(logs)\n",
        "            tx_hashes = [log[\"transactionHash\"] for log in logs if log[\"topics\"][1].lower() != f\"0x000000000000000000000000{user_address[2:].lower()}\"]\n",
        "            return list(set(tx_hashes))\n",
        "        except Exception as e:\n",
        "            #print(f\"Error for get hash of {user_address}: {str(e)}\")\n",
        "            #time.sleep(1)\n",
        "            return []\n",
        "\n",
        "def get_block_timestamp(block_number, api_key):\n",
        "    url = f\"https://api.polygonscan.com/api?module=block&action=getblockreward&blockno={block_number}&apikey={api_key}\"\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()[\"result\"]\n",
        "            if data:\n",
        "                return int(data[\"timeStamp\"])\n",
        "            else:\n",
        "                return None\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error for get timestamp {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "def get_transaction_data(user_address, tx_hash, block_timestamp_cache, api_key):\n",
        "    url = f\"https://api.polygonscan.com/api?module=proxy&action=eth_getTransactionReceipt&txhash={tx_hash}&apikey={api_key}\"\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            time.sleep(1 / RATE_LIMIT)\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if not data:\n",
        "                return None\n",
        "\n",
        "            if \"result\" not in data:\n",
        "                print(f\"No 'result' field in response for tx hash {tx_hash}\")\n",
        "                return None\n",
        "\n",
        "            tx_data = data[\"result\"]\n",
        "\n",
        "            logs = tx_data[\"logs\"]\n",
        "\n",
        "            has_target_topic = False\n",
        "            for log in logs:\n",
        "                if log[\"topics\"][0].lower() == topic_0:\n",
        "                    has_target_topic = True\n",
        "                    break\n",
        "\n",
        "            if not has_target_topic:  # Skip split/merge tx\n",
        "                return None\n",
        "\n",
        "            block_number = int(tx_data[\"blockNumber\"], 16)  # Reduce the use of API2\n",
        "            if block_number in block_timestamp_cache:\n",
        "                timestamp = block_timestamp_cache[block_number]\n",
        "            else:\n",
        "                timestamp = get_block_timestamp(block_number, api_key)\n",
        "                block_timestamp_cache[block_number] = timestamp\n",
        "\n",
        "            activity_type = None\n",
        "            size = None\n",
        "            usdc_size = None\n",
        "            token_id = None\n",
        "            outcome_index = None\n",
        "\n",
        "            for log in logs:\n",
        "                topics = log[\"topics\"]\n",
        "                log_data = log[\"data\"]\n",
        "\n",
        "                if topics[0].lower() == \"0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef\":  # ERC20 Transfer event\n",
        "                    #print(\"Processing Transfer event\")\n",
        "                    if topics[1].lower() == f\"0x000000000000000000000000{user_address[2:].lower()}\":  # User is sender\n",
        "                        activity_type = \"Sell\"\n",
        "                    elif topics[2].lower() == f\"0x000000000000000000000000{user_address[2:].lower()}\":  # User is recipient\n",
        "                        activity_type = \"Buy\"\n",
        "                    usdc_size = int(log_data, 16) / 10**6  # USDC has 6 decimal places\n",
        "\n",
        "                elif topics[0].lower() == \"0xc3d58168c5ae7397731d063d5bbf3d657854427343f4c083240f7aacaa2d0f62\":  # ERC1155 TransferSingle event\n",
        "                    if topics[2].lower() == f\"0x000000000000000000000000{user_address[2:].lower()}\":  # User is sender\n",
        "                        activity_type = \"Sell\"\n",
        "                    elif topics[3].lower() == f\"0x000000000000000000000000{user_address[2:].lower()}\":  # User is recipient\n",
        "                        activity_type = \"Buy\"\n",
        "                    token_id = int(log_data[:66], 16)\n",
        "                    size = int(log_data[66:], 16) / 10**6  # 6 decimal places for size\n",
        "\n",
        "                elif topics[0].lower() == \"0x4f62630f51608fc8a7603a9391a5101e58bd7c276139366fc107dc3b67c3dcf8\":  # Custom event for outcome index\n",
        "                    outcome_index = int(topics[2], 16)\n",
        "\n",
        "            if activity_type is not None and size is not None and usdc_size is not None:\n",
        "                if size != 0:\n",
        "                    price = usdc_size / size\n",
        "                else:\n",
        "                    price = 0\n",
        "\n",
        "                activity = {\n",
        "                    \"timestamp\": timestamp,\n",
        "                    \"side\": activity_type,\n",
        "                    \"size\": size,\n",
        "                    \"usdcSize\": usdc_size,\n",
        "                    \"transactionHash\": str(tx_hash),\n",
        "                    \"price\": price,\n",
        "                    \"asset\": str(token_id)\n",
        "                }\n",
        "\n",
        "                if outcome_index is not None:\n",
        "                    activity[\"outcomeIndex\"] = outcome_index\n",
        "\n",
        "                return activity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error for get tx {tx_hash} for {user_address}: {str(e)}\")\n",
        "            time.sleep(1)\n",
        "            return None\n",
        "\n",
        "def process_user_transactions(user_address, block_step=20000000, block_timestamp_cache=None, api_key=None):\n",
        "\n",
        "    if block_timestamp_cache is None:\n",
        "        block_timestamp_cache = {}\n",
        "    all_activities = []\n",
        "    all_activities_lock = threading.Lock()\n",
        "\n",
        "    local_existing_hashes = set(exist_hash.get(user_address, []))\n",
        "\n",
        "    num_intervals = (end_block - start_block) // block_step + 1\n",
        "    for i in range(num_intervals):\n",
        "        from_block = start_block + i * block_step\n",
        "        to_block = min(from_block + block_step - 1, end_block)\n",
        "        try:\n",
        "            tx_hashes = get_transaction_hashes(user_address, from_block, to_block, api_key)\n",
        "            new_hashes = set(tx_hashes) - local_existing_hashes\n",
        "\n",
        "            for tx_hash in new_hashes:\n",
        "                activity = None  # Reset activity\n",
        "                try:\n",
        "                    activity = get_transaction_data(user_address, tx_hash, block_timestamp_cache, api_key)\n",
        "                    if activity:\n",
        "                        with all_activities_lock:\n",
        "                            all_activities.append(activity)\n",
        "                finally:\n",
        "                    activity = None  # Reset activity regardless of whether an exception occurred\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing {user_address}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    print(f\"{len(all_activities)} new tx for {user_address}\")\n",
        "    return all_activities\n",
        "\n",
        "def process_user_transactions_parallel(user_addresses, max_workers=6):\n",
        "    result_queue = queue.Queue()\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {}\n",
        "        api_key_index = 0\n",
        "        for user_address in user_addresses:\n",
        "            api_key = api_keys[api_key_index]\n",
        "            block_timestamp_cache = {}\n",
        "            future = executor.submit(process_user_transactions, user_address, block_timestamp_cache=block_timestamp_cache, api_key=api_key)\n",
        "            futures[future] = user_address\n",
        "            api_key_index = (api_key_index + 1) % len(api_keys)  # Cycle through the API keys\n",
        "\n",
        "        completed_count = 0\n",
        "        total_count = len(user_addresses)\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            user_address = futures[future]\n",
        "            user_transactions = future.result()\n",
        "            if user_transactions:\n",
        "                user_result = {\n",
        "                    \"proxyWallet\": user_address,\n",
        "                    \"activities\": user_transactions\n",
        "                }\n",
        "                result_queue.put(user_result)\n",
        "            completed_count += 1\n",
        "            progress = completed_count / total_count * 100\n",
        "            print(f\"Progress: {completed_count}/{total_count} ({progress:.2f}%)\")\n",
        "\n",
        "    add_tx = []\n",
        "    while not result_queue.empty():\n",
        "        add_tx.append(result_queue.get())\n",
        "\n",
        "    return add_tx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moH1w4bntHa4"
      },
      "outputs": [],
      "source": [
        "add_tx = process_user_transactions_parallel(list(exist_hash.keys()))\n",
        "#add_tx to json\n",
        "with open('./data/temp/add_tx.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(add_tx, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCUh56CW373g"
      },
      "outputs": [],
      "source": [
        "with open('./data/temp/add_tx.json', 'r', encoding='utf-8') as file:\n",
        "    add_tx = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z65DZAEyI3aH",
        "outputId": "79c30a9e-87ff-4ef9-827c-08a51c9a1c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of added proxyWallets: 5547\n"
          ]
        }
      ],
      "source": [
        "# Create dictionaries for faster matching\n",
        "asset_to_position = {}\n",
        "asset_to_activity = {}\n",
        "\n",
        "for profile_url, profile_data in politics_users_profile.items():\n",
        "    for position in profile_data['positions']:\n",
        "        asset_to_position[position['asset']] = {\n",
        "            'conditionId': position['conditionId'],\n",
        "            'title': position['title'],\n",
        "            'slug': position['slug'],\n",
        "            'eventSlug': position['eventSlug'],\n",
        "            'outcome': position['outcome'],\n",
        "            'outcomeIndex': position['outcomeIndex']\n",
        "        }\n",
        "        if 'oppositeAsset' in position:\n",
        "            asset_to_position[position['oppositeAsset']] = {\n",
        "                'conditionId': position['conditionId'],\n",
        "                'title': position['title'],\n",
        "                'slug': position['slug'],\n",
        "                'eventSlug': position['eventSlug'],\n",
        "                'outcome': 'Yes' if position['outcome'] == 'No' else 'No',\n",
        "                'outcomeIndex': 0 if position['outcomeIndex'] == 1 else 1\n",
        "            }\n",
        "    for activity in profile_data['activities']:\n",
        "        asset_to_activity[activity['asset']] = activity\n",
        "\n",
        "# Iterate over each activity in add_tx\n",
        "for wallet in add_tx:\n",
        "    for activity in wallet['activities']:\n",
        "        activity['type'] = 'TRADE'\n",
        "        asset = activity['asset']\n",
        "\n",
        "        # Adjust the value of \"side\" to uppercase\n",
        "        activity['side'] = activity['side'].upper()\n",
        "\n",
        "        # Search for matching asset in positions\n",
        "        if asset in asset_to_position:\n",
        "            position = asset_to_position[asset]\n",
        "            activity.update(position)\n",
        "        # Search for matching asset in activities\n",
        "        elif asset in asset_to_activity:\n",
        "            matched_activity = asset_to_activity[asset]\n",
        "            activity.update({\n",
        "                #'timestamp': matched_activity['timestamp'],\n",
        "                'conditionId': matched_activity['conditionId'],\n",
        "                'title': matched_activity['title'],\n",
        "                'slug': matched_activity['slug'],\n",
        "                'eventSlug': matched_activity['eventSlug'],\n",
        "                'outcome': matched_activity['outcome'],\n",
        "                'outcomeIndex': matched_activity['outcomeIndex']\n",
        "            })\n",
        "\n",
        "    # Remove activities without conditionId or with null timestamp\n",
        "    wallet['activities'] = [activity for activity in wallet['activities'] if 'conditionId' in activity and activity['timestamp'] is not None]\n",
        "\n",
        "# Remove proxyWallets with empty activities\n",
        "add_tx = [wallet for wallet in add_tx if wallet['activities']]\n",
        "\n",
        "print(f\"Total number of added proxyWallets: {len(add_tx)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egmcLjGvM03I"
      },
      "outputs": [],
      "source": [
        "# Merge processed add_tx data into politics_users_profile\n",
        "for wallet in add_tx:\n",
        "    proxy_wallet = wallet['proxyWallet']\n",
        "    profile_url = f\"https://polymarket.com/profile/{proxy_wallet}\"\n",
        "    if profile_url in politics_users_profile:\n",
        "        politics_users_profile[profile_url]['activities'].extend(wallet['activities'])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "# Sort activities by timestamp for each proxyWallet\n",
        "for profile_data in politics_users_profile.values():\n",
        "    profile_data['activities'].sort(key=lambda x: x['timestamp'])\n",
        "\n",
        "# Write the updated politics_users_profile to a file\n",
        "with open('./data/politics_users_profile_api2.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(politics_users_profile, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usffUhIJ9JKV"
      },
      "source": [
        "### Subgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbC-Hq2ROq7D"
      },
      "outputs": [],
      "source": [
        "url = \"https://api.studio.thegraph.com/query/63814/pm_analysis/v0.0.1\"\n",
        "log_file1 = './data/temp/account_query_errors.log'\n",
        "log_file2 = './data/temp/tx_query_errors.log'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88DlA5sdN2xG"
      },
      "outputs": [],
      "source": [
        "def send_request(url, query, account_id, skip, log_file, max_retries=3):\n",
        "    last_error = ''  # Record the last error\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, json={'query': query}, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            data = json.loads(response.text)\n",
        "\n",
        "            # For no data obtained\n",
        "            if not data.get('data', {}).get('account'):\n",
        "                last_error = f\"No data retrieved for: {account_id}\"\n",
        "                continue # Again\n",
        "\n",
        "            return data  # Success\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            last_error = f\"Failed to fetch: {account_id}, error: {e}\"\n",
        "            continue  # Again\n",
        "\n",
        "    # Re-try 3 times，record error\n",
        "    with open(log_file, 'a') as log:\n",
        "        log.write(last_error + \"\\n\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hiy1-HJAO5OK"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for account_id in unique_accounts:\n",
        "    skip = 0  # original skip\n",
        "    all_fpmm_memberships = []  # The liquidities where the address provided, corresponding to the marketMakerAddress\n",
        "\n",
        "    while True:\n",
        "        query = \"\"\"\n",
        "        {{\n",
        "          account(id: \"{account_id}\") {{\n",
        "            id\n",
        "            creationTimestamp\n",
        "            lastSeenTimestamp\n",
        "            lastTradedTimestamp\n",
        "            numTrades\n",
        "            scaledProfit\n",
        "            scaledCollateralVolume\n",
        "            fpmmPoolMemberships(skip: {skip}) {{\n",
        "              id\n",
        "              amount\n",
        "              pool {{\n",
        "                    scaledFeeVolume\n",
        "                    scaledLiquidityParameter\n",
        "                    liquidityAddQuantity\n",
        "              }}\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        \"\"\".format(account_id=account_id, skip=skip)\n",
        "\n",
        "        data = send_request(url, query, account_id, skip, log_file1)\n",
        "        if data is None or not data['data'].get('account'):\n",
        "            break # For errors or totally null\n",
        "\n",
        "        account_data = data['data']['account']\n",
        "        fpmm_memberships = account_data.get('fpmmPoolMemberships', [])\n",
        "\n",
        "        for membership in fpmm_memberships:\n",
        "            membership['id'] = membership['id'].replace(account_id, '')\n",
        "\n",
        "        all_fpmm_memberships.extend(fpmm_memberships)\n",
        "        if not fpmm_memberships:\n",
        "            break  # No more fpmmPoolMemberships data\n",
        "\n",
        "        skip += 100  # More data\n",
        "\n",
        "    account_data['fpmmPoolMemberships'] = all_fpmm_memberships\n",
        "    results.append(account_data)\n",
        "\n",
        "# Save JSON\n",
        "with open('./data/temp/politics_users_profile_infor.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(results, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwEf70Mfio-f"
      },
      "outputs": [],
      "source": [
        "# Re-retrieve\n",
        "with open('./data/temp/politics_users_profile_infor.json', 'r', encoding='utf-8') as file:\n",
        "    existing_results = json.load(file)\n",
        "existing_results_map = {item['id']: item for item in existing_results}\n",
        "\n",
        "# From log_file1\n",
        "with open(log_file1, 'r') as file:\n",
        "    failed_accounts_fpmm = {line.split(\": \")[1].strip() for line in file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpWQ3C7tgy0J"
      },
      "outputs": [],
      "source": [
        "# re-retrieve for failed accounts\n",
        "results = []\n",
        "for account_id in failed_accounts_fpmm:\n",
        "\n",
        "    skip = 0\n",
        "    all_fpmm_memberships = []\n",
        "\n",
        "    while True:\n",
        "        query = \"\"\"\n",
        "        {{\n",
        "          account(id: \"{account_id}\") {{\n",
        "            id\n",
        "            creationTimestamp\n",
        "            lastSeenTimestamp\n",
        "            lastTradedTimestamp\n",
        "            numTrades\n",
        "            scaledProfit\n",
        "            scaledCollateralVolume\n",
        "            fpmmPoolMemberships(skip: {skip}) {{\n",
        "              id\n",
        "              amount\n",
        "              pool {{\n",
        "                    scaledFeeVolume\n",
        "                    scaledLiquidityParameter\n",
        "                    liquidityAddQuantity\n",
        "              }}\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        \"\"\".format(account_id=account_id, skip=skip)\n",
        "\n",
        "        data = send_request(url, query, account_id, skip, log_file1)\n",
        "        if data is None or not data['data'].get('account'):\n",
        "            break # For errors or totally null\n",
        "\n",
        "        account_data = data['data']['account']\n",
        "        fpmm_memberships = account_data.get('fpmmPoolMemberships', [])\n",
        "\n",
        "        for membership in fpmm_memberships:\n",
        "            membership['id'] = membership['id'].replace(account_id, '')\n",
        "\n",
        "        all_fpmm_memberships.extend(fpmm_memberships)\n",
        "        if not fpmm_memberships:\n",
        "            break  # No more fpmmPoolMemberships data\n",
        "\n",
        "        skip += 100  # More data\n",
        "\n",
        "    account_data['fpmmPoolMemberships'] = all_fpmm_memberships\n",
        "    results.append(account_data)\n",
        "\n",
        "for account_data in results:\n",
        "    account_id = account_data['id']\n",
        "    existing_results_map[account_id] = account_data\n",
        "\n",
        "updated_results = list(existing_results_map.values())\n",
        "\n",
        "with open('./data/temp/updated_politics_users_profile_infor.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(updated_results, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Gtk2NCroS_5"
      },
      "outputs": [],
      "source": [
        "# retrieve for transactions\n",
        "results = []\n",
        "\n",
        "for account_id in unique_accounts:\n",
        "    skip = 0  # original skip\n",
        "    all_transactions = []\n",
        "    account_basic_info = None  # The ID\n",
        "    while True:\n",
        "        query = \"\"\"\n",
        "        {{\n",
        "          account(id: \"{account_id}\") {{\n",
        "            id\n",
        "            transactions(skip: {skip}) {{\n",
        "              id\n",
        "              type\n",
        "              tradeAmount\n",
        "              timestamp\n",
        "              outcomeTokensAmount\n",
        "              outcomeIndex\n",
        "              market {{\n",
        "                id\n",
        "              }}\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        \"\"\".format(account_id=account_id, skip=skip)\n",
        "\n",
        "        data = send_request(url, query, account_id, skip, log_file2)\n",
        "        if data is None or not data['data'].get('account'):\n",
        "            break  # Error or Null\n",
        "\n",
        "        account_data = data['data']['account']\n",
        "        if skip == 0:  # At the first time, record the ID\n",
        "            if not account_data.get('id'):\n",
        "                break\n",
        "            account_basic_info = {'id': account_data['id']}\n",
        "\n",
        "        transactions = account_data.get('transactions', [])\n",
        "        if not transactions:\n",
        "            break  # No more tx data\n",
        "\n",
        "        all_transactions.extend(transactions)\n",
        "        skip += len(transactions)  # More data\n",
        "    if account_basic_info:\n",
        "        account_basic_info['transactions'] = all_transactions\n",
        "        results.append(account_basic_info)\n",
        "\n",
        "# Save JSON\n",
        "with open('./data/temp/politics_users_profile_tx.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(results, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poPwNjTInLwg"
      },
      "outputs": [],
      "source": [
        "# Re-retrieve tx\n",
        "with open('./data/temp/politics_users_profile_tx.json', 'r', encoding='utf-8') as file:\n",
        "    existing_tx_results = json.load(file)\n",
        "existing_tx_map = {item['id']: item for item in existing_tx_results}\n",
        "\n",
        "with open(log_file2, 'r') as file:\n",
        "    failed_accounts_tx = {line.split(\": \")[1].strip() for line in file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FthFbFvcndAV"
      },
      "outputs": [],
      "source": [
        "# re-retrieve for failed tx\n",
        "for account_id in failed_accounts_tx:\n",
        "    skip = 0\n",
        "    all_transactions = []\n",
        "    while True:\n",
        "        query = \"\"\"\n",
        "        {{\n",
        "          account(id: \"{account_id}\") {{\n",
        "            id\n",
        "            transactions(skip: {skip}) {{\n",
        "              id\n",
        "              type\n",
        "              tradeAmount\n",
        "              timestamp\n",
        "              outcomeTokensAmount\n",
        "              outcomeIndex\n",
        "              market {{\n",
        "                id\n",
        "              }}\n",
        "            }}\n",
        "          }}\n",
        "        }}\n",
        "        \"\"\".format(account_id=account_id, skip=skip)\n",
        "\n",
        "        data = send_request(url, query, account_id, skip, log_file2)\n",
        "        if data is None or not data['data'].get('account'):\n",
        "            break\n",
        "\n",
        "        account_data = data['data']['account']\n",
        "        transactions = account_data.get('transactions', [])\n",
        "        if not transactions:\n",
        "            break\n",
        "\n",
        "        all_transactions.extend(transactions)\n",
        "        skip += len(transactions)\n",
        "\n",
        "    if account_id in existing_tx_map:\n",
        "        existing_tx_map[account_id]['transactions'].extend(all_transactions)\n",
        "    else:\n",
        "        existing_tx_map[account_id] = {'id': account_id, 'transactions': all_transactions}\n",
        "\n",
        "updated_tx_results = list(existing_tx_map.values())\n",
        "\n",
        "# Save JSON\n",
        "with open('./data/temp/updated_politics_users_profile_tx.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(updated_tx_results, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoxdMgAZrUx_"
      },
      "outputs": [],
      "source": [
        "# Combine data\n",
        "with open('./data/temp/updated_politics_users_profile_infor.json', 'r', encoding='utf-8') as file:\n",
        "    profile_data = json.load(file)\n",
        "with open('./data/temp/updated_politics_users_profile_tx.json', 'r', encoding='utf-8') as file:\n",
        "    transactions_data = json.load(file)\n",
        "\n",
        "# maping transactions to id\n",
        "transactions_map = {item['id']: item for item in transactions_data}\n",
        "\n",
        "for item in profile_data:\n",
        "    account_id = item['id']\n",
        "    if account_id in transactions_map:\n",
        "        item['transactions'] = transactions_map[account_id]['transactions']\n",
        "    else:\n",
        "        item['transactions'] = []\n",
        "\n",
        "# Save final JSON, to combine with data obtained from Polygon API\n",
        "with open('./data/temp/politics_users_profile.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(profile_data, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb0le0XOdqyq"
      },
      "source": [
        "### Preprocessing for outcomes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTc8XTmqMp2p"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "with open('./data/events_details.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-Ebm5GwMvHw"
      },
      "outputs": [],
      "source": [
        "result = {}\n",
        "\n",
        "for url, event_data in data.items():\n",
        "    try:\n",
        "        markets = event_data['props']['pageProps']['dehydratedState']['queries'][0]['state']['data']['markets']\n",
        "        for market in markets:\n",
        "            slug = market.get('slug', None)\n",
        "            end_date = market.get('endDate', None)\n",
        "            start_date = market.get('startDate', None)\n",
        "            created_at = market.get('createdAt', None)\n",
        "            closed = market.get('closed', None)\n",
        "            closed_time = market.get('closedTime', None)\n",
        "            resolution_price = market.get('resolutionData', {}).get('price', None)\n",
        "\n",
        "            result[slug] = {\n",
        "                'endDate': end_date,\n",
        "                'startDate': start_date,\n",
        "                'createdAt': created_at,\n",
        "                'closed': closed,\n",
        "                'closedTime': closed_time,\n",
        "                'resolutionPrice': resolution_price\n",
        "            }\n",
        "    except (KeyError, IndexError):\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkyEF6JSNOKw"
      },
      "outputs": [],
      "source": [
        "# Save JSON\n",
        "with open('./data/events_details_clip.json', 'w', encoding='utf-8') as file:\n",
        "    json.dump(result, file, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3osYVr3SpBLL"
      },
      "outputs": [],
      "source": [
        "filtered_data = {}\n",
        "\n",
        "for slug, data in result.items():\n",
        "    resolution_price = data['resolutionPrice']\n",
        "    if resolution_price not in [\"1000000000000000000\", \"0\"]:\n",
        "        filtered_data[slug] = data\n",
        "\n",
        "urls = [\"https://polymarket.com/market/\" + slug for slug in filtered_data.keys()]\n",
        "\n",
        "with open('./data/temp/output.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['URL'])\n",
        "    writer.writerows([[url] for url in urls])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4FrdVblkwYI"
      },
      "outputs": [],
      "source": [
        "with open('./data/events_details_clip.json', 'r', encoding='utf-8') as file:\n",
        "    result = json.load(file)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "D7y_ts92fMKY",
        "nkvKqc2OwPY3",
        "lu3F7jJp3wEV",
        "TSWWycIgyaCO"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
